{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-최근접 이웃(K-Nearest Neighbor, KNN)이란, 가장 가까운(가장 유사한) k개의 데이터 포인트(이웃)를 찾아 새로운 데이터 포인트를 예측하는 비모수적 예측방법으로, 회귀와 분류 문제에 모두 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knn](https://www.dropbox.com/s/6scqv9dt9voikhw/knn1.jpg?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림은 K = 3인 분류 문제의 예시인데, 가장 가까운 3개의 이웃을 찾아 voting에 의해 분류를 한다.\n",
    "* 회귀 : k개 이웃의 평균\n",
    "* 분류 : k개 이웃 중 가장 많이 나온 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN은 모델을 학습하지 않고, 새로운 데이터 포인트가 들어올 때마다 기존 데이터와 거리를 각각 계산하여 예측하기 때문에 게으른 학습(Lazy Learning), 혹은 Instance-based Learning이라고 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도 또는 거리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 데이터 포인트와 유사한 이웃을 찾기 위해서는 유사도(Similarity) 또는 거리(Distance)를 계산해야한다. 일반적으로 연속형 변수에서 많이 사용하는 거리 척도는 Euclidean Distance이다. 범주형 변수가 있을 경우에는 Hamming Distance를 사용하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manhattan Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minkowski Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hamming Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규화(Normalization)\n",
    "변수의 단위에 따라 거리에 미치는 영향력이 다르기 때문에 정규화를 통해 변수를 변환하여 사용\n",
    "* 최소-최대 정규화 : 데이터의 최대/최소 값을 이용하여 모든 속성 값을 0과 1 사이의 값으로 변환\n",
    "$$ Xnew = \\frac{X - Xmin}{Xmax - Xmin}$$\n",
    "* 표준화(Standardazation) : 데이터의 평균을 빼고 데이터의 표준 편차로 나누어 변환\n",
    "$$ Xnew = \\frac{X - Xmean}{Xsd}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN은 탐색할 이웃의 수(K)를 선택해야한다. 이때 얼마나 많은 이웃(K)을 사용할지 결정하는 것은 데이터에 의존적이다. K를 작게 설정할 경우에는 Overfitting이 발생하고, 반대로 K를 너무 크게 설정할 경우에는 과하게 일반화(Generalization)되어 Underfitting이 발생하기 때문에 적절한 K를 찾는 것이 중요하다. 여러가지 K값을 시도해보고 평가 척도가 가장 좋은 K값을 선택하는데, 일반적으로 n:n으로 동률의 투표가 나올 것을 방지하기 위해 홀수인 K를 선택한다.\n",
    "* 너무 작은 K : Overfitting의 우려\n",
    "* 너무 큰 K : Outlier의 영향이 줄어들지만, 항목간 경계가 불문명해짐(Underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 적절한  데이터로 knn 수행 후 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "library(caret)\n",
    "library(kknn)\n",
    "\n",
    "# Load data\n",
    "data(iris)\n",
    "\n",
    "# Split data\n",
    "#set.seed(1990)\n",
    "train_idx <- createDataPartition(iris$Species, p = 0.7, list = F)\n",
    "train <- iris[train_idx,]\n",
    "test <- iris[-train_idx,]\n",
    "\n",
    "# Train\n",
    "knn_1 <- kknn(Species ~ ., train, test, k = 1, distance = 2)\n",
    "knn_3 <- kknn(Species ~ ., train, test, k = 3, distance = 2)\n",
    "knn_5 <- kknn(Species ~ ., train, test, k = 5, distance = 2)\n",
    "knn_7 <- kknn(Species ~ ., train, test, k = 7, distance = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제점들\n",
    "* 차원의 저주 : 차원이 너무 많아서 최근접이웃이라 해도 현실적으로는 '가깝다'라고 하기엔 너무 멈\n",
    "* 과적합 : 어떤 이웃이 갖아 가까워도 그 이웃이 완전히 잡음(Noise)일 수 있음(K를 증가시키면 잡음의 영향을 줄일 수 있음)\n",
    "* 변수들간의 상관관계 : 변수들이 매우 많기 때문에 이들 간에 서로 높은 상관을 가진 변수가 있을 수 있음(상관에 대한 이해를 바탕으로 적은 차원의 공간으로 투영)\n",
    "* 변수들의 상대적 중요성 : 어떤 변수들은 다른 변수보다 중요할 수 있음(변수에 가중치를 부여)\n",
    "* 희소 : 벡터 또는 행렬이 희소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과반수 의결”에 따른 분류의 단점은 항목 분포가 편향될 때 나타난다. 다시 말해서, 더 빈번한 항목의 데이터가 새로운 데이터의 예측을 지배하는 경향이 있다. 왜냐하면, 더 빈번한 항목의 데이터가 다수를 이루어서 k개의 최근접 이웃의 대부분이 되는 경향이 있기 때문이다.[4] 이 문제를 해결하는 한 가지 방법은 검증점과 k개의 최근접 이웃 각각의 거리를 고려하여 분류에 가중치를 주는 것이다. 각각 k개의 최근접 점의 항목(회귀 문제의 경우 값)에 그 점에서 검증점까지의 거리에 반비례하는 가중치를 곱한다. 편향을 해결하는 또 다른 방법은 데이터 표현을 추상화하는 것이다. 예를 들어 자기 조직화 지도(SOM)에서 각 노드는 원래의 훈련 데이터 밀도와 상관없이 유사점의 군집 대표(중심)이다. K-NN은 SOM에도 적용될 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
