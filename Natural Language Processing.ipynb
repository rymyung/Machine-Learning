{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP(Natural Language Processing, 자연어처리)란 '컴퓨터가 인간이 사용하는 언어를 이해하고, 분석할 수 있게 하는 분야'를 나타내는 말이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bag of word\n",
    "    * one-hot encoding\n",
    "    * word count\n",
    "    * tf-idf\n",
    "    * N-grams\n",
    "  \n",
    "  \n",
    "* word embedding\n",
    "    * word2vec\n",
    "    * doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of word(one-hot encoding)\n",
    "단어들을 사용해서 무언가 모델을 만들 때, 가장 단순한 방법은 bag-of-word(one-hot-encoding)이다.  \n",
    "단어가 있으면 1, 없으면 0으로 나타내어 사전이 [case, word, bag]이라면 [1, 0, 0]으로 표현한다. 하지만 그러한 encoding을 사용하면 단어들 간의 관계(유의어, 반의어 등)을 나타내지 못한다. paris-france / paris-case를 똑같은 관계로 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is used to generate representation vectors out of words.  \n",
    "Word2Vec은 각각의 word를 numeric representation으로 나타내는 것이다. 그렇게 나타내므로써 word 간의 관계를 찾아낼 수 있다.  \n",
    "Word2Vec representation은 2개의 알고리즘을 사용해 만들어진다.\n",
    "* Continuous bag of words(CBOW)\n",
    "* Skip-Gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous bag of words(CBOW)\n",
    "CBOW 모델의 경우 다음과 같은 방식을 채용하고 있하고 생각할 수 있을 것 같다. “집 앞 편의점에서 아이스크림을 사 먹었는데, \\_\\_\\_ 시려서 너무 먹기가 힘들었다.” 라는 문장에서, 사람들은 ___ 부분에 들어갈 단어가 정확하게 주어지지 않아도 앞 뒤의 단어들을 통해 ‘이가’ 라는 말이 들어갈 것을 추측할 수 있다. CBOW 모델도 마찬가지의 방법을 사용한다. 주어진 단어에 대해 앞 뒤로 C/2개 씩 총 C개의 단어를 Input으로 사용하여, 주어진 단어를 맞추기 위한 네트워크를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram model\n",
    "Skip-Gram 모델은 CBOW의 반대이다. 한 단어를 예측하는 것 대신, 모든 주변 단어들(='context')을 예측하기 위해 하나의 단어를 사용한다. 주어진 단어 하나를 가지고 주위에 등장하는 나머지 몇 가지의 단어들의 등장여부를 유추하는 것이다. skip-gram 모델은 CBOW에 비해 훨씬 느린 반면, 많이 나오지 않는 단어들에 대해 훨씬 더 정확하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec\n",
    "Doc2Vec은 document(혹은 sentence)의 길이에 상관없이 document의 numeric representation를 만드는 것이다.\n",
    "* PV-DM\n",
    "* PV-DBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV-DM\n",
    "PV-DM(Distributed Memory version of Paragraph Vector)이란 Word2Vec 모델을 사용하면서, 다른 vector(Paragraph ID, document unique)를 추가하는 방법이다.(CBOW + paragraph ID vector)  \n",
    "It acts as a memory that remembers what is missing from the current context — or as the topic of the paragraph.  \n",
    "While the word vectors represent the concept of a word, the document vector intends to represent the concept of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV-DBOW\n",
    "PV-DBOW(Distributed Bag of Words version of Paragraph Vector)란, ~이다.  \n",
    "this algorithm is actually faster (as opposed to word2vec) and consumes less memory, since there is no need to save the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
